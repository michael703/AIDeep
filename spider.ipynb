{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 爬虫作业"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 必要包导入\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 用于启动浏览器驱动\n",
    "from selenium import webdriver\n",
    "# 过滤筛选\n",
    "from selenium.webdriver.common.by import By\n",
    "# 页面解析\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "若没装driver则在终端中运行如下代码进行安装\n",
    "\n",
    "$ brew install chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 设置搜索关键词\n",
    "keyword = '疫情下'\n",
    "# 帖子标题 发帖人 发帖时间 link 每个帖子的评论 评论时间\n",
    "# 初始化记录项\n",
    "results = dict(\n",
    "    帖子标题=[],\n",
    "    发帖人=[],\n",
    "    发帖时间=[],\n",
    "    link=[],\n",
    "    评论=[],\n",
    "    评论时间=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 网站一 babykindom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 网站配置\n",
    "\n",
    "URL: https://www.baby-kingdom.com/search.php?mod=forum&searchid=1658033935&srchtxt={关键词}&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword={关键词}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "host = 'https://www.baby-kingdom.com/'\n",
    "url = 'search.php?mod=forum&searchid=1658033935&srchtxt=关键词&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=关键词'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 初始化浏览器 并加载搜索关键词页\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(f'{host}{url.replace(\"关键词\", keyword)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 获取所有帖子链接\n",
    "html = driver.page_source\n",
    "soup = bs(html,'html.parser')\n",
    "links = []\n",
    "contents = soup.find_all('h3', {'class': 'xs3'})\n",
    "next_page = soup.find('a', {'class': 'nxt'})\n",
    "for content in contents:\n",
    "    content_link = list(content.children)[0]['href']\n",
    "    links.append(content_link)\n",
    "while next_page:\n",
    "    driver = webdriver.Chrome()\n",
    "    try:\n",
    "        driver.get(f\"{host}{next_page['href']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{next_page['href']} 请求失败 正在重试\")\n",
    "        try:\n",
    "            time.sleep(3)\n",
    "            driver.get(f\"{host}{next_page['href']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{next_page['href']} 请求失败 跳过请求\")\n",
    "            driver.close()\n",
    "            time.sleep(3)\n",
    "            continue\n",
    "    html = driver.page_source\n",
    "    soup = bs(html,'html.parser')\n",
    "    next_page = soup.find('a', {'class': 'nxt'})\n",
    "    for content in contents:\n",
    "        content_link = list(content.children)[0]['href']\n",
    "        links.append(content_link)\n",
    "print(f'获取链接结束, 共计 { len(links)}')\n",
    "for link in links:\n",
    "    print(link)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "saved_links = links.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "links = saved_links.copy()\n",
    "len(links)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(links))\n",
    "author = ''\n",
    "while links:\n",
    "    # 进入每个帖子 获取帖子信息\n",
    "    link = links.pop(0)\n",
    "    driver = webdriver.Chrome()\n",
    "    try:\n",
    "        driver.get(link)\n",
    "    except Exception as e:\n",
    "        print(f'加载失败{link}，正在重试')\n",
    "        try:\n",
    "            driver.get(link)\n",
    "        except Exception:\n",
    "            print(f'加载失败，跳过; 剩余链接: {len(links)}')\n",
    "            driver.close()\n",
    "            continue\n",
    "    html = driver.page_source\n",
    "    soup = bs(html,'html.parser')\n",
    "    title = ''.join(soup.find('a', {'id': 'thread_subject'}).strings)\n",
    "    if not author:\n",
    "        author = ''.join(soup.find('div', {'class', 'authi'}).strings).strip()\n",
    "    author_time = ''.join(list(list(list(soup.find_all('div', {'class', 'authi'}))[1].children)[3].children)[1].strings)\n",
    "    print(title, author, author_time, link)\n",
    "    content_list = soup.find_all('div', {'class': 't_fsz'})\n",
    "    comment_texts = []\n",
    "    comment_times = []\n",
    "    for content in content_list:\n",
    "        text = ''.join(content.find('span').stripped_strings)\n",
    "        text = re.sub('\\d\\d-\\d+-\\d\\d \\d\\d:\\d\\d', '', text)\n",
    "        text = re.sub('上傳下載附件\\(.*\\)', '', text)\n",
    "        text = re.sub('\\s+發表於 ', '', text)\n",
    "        text = re.sub('本帖最後由 .+ 於  編輯', '', text)\n",
    "        comment_texts.append(text)\n",
    "    comment_list = soup.find_all('div', {'class', 'authi'})\n",
    "    for comment in comment_list:\n",
    "        comment_time_string = ''.join(comment.stripped_strings)\n",
    "        comment_time = re.findall('\\d\\d-\\d+-\\d\\d \\d\\d:\\d\\d', comment_time_string)\n",
    "        if not comment_time:\n",
    "            continue\n",
    "        comment_times.append(comment_time)\n",
    "    \n",
    "    # 保存数据\n",
    "    for text, comment_time in zip(comment_texts, comment_times):\n",
    "        results['帖子标题'].append(title)\n",
    "        results['发帖人'].append(author)\n",
    "        results['发帖时间'].append(author_time)\n",
    "        results['link'].append(link)\n",
    "        results['评论'].append(text)\n",
    "        results['评论时间'].append(comment_time)\n",
    "    \n",
    "    \n",
    "    has_next = soup.find('div', {'class', 'pagination-control pagination-nextpage'})\n",
    "    if has_next:\n",
    "        if has_next.find('a'):\n",
    "            link = f\"{host}{has_next.find('a')['href']}\"\n",
    "            links = [link] + links\n",
    "        else:\n",
    "            author = ''\n",
    "    else:\n",
    "        author = ''\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.replace('',np.nan, inplace =True)\n",
    "df.dropna(subset=['评论'], inplace =True)\n",
    "df.to_excel(f'网站1结果_{keyword}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 网站二 hk高登"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 网站配置\n",
    "\n",
    "URL: https://m.hkgolden.com/search/T/{关键词}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 网站2配置\n",
    "host_2 = 'https://m.hkgolden.com'\n",
    "url_2 = '/search/T/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "帖子标题 发帖人 发帖时间 link 每个帖子的评论 评论时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 加载网站2入口\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(host_2+url_2+keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "html_2 = driver.page_source\n",
    "soup2 = bs(html_2,'html.parser')\n",
    "end_tag = soup2.find('div', {'class': 'jss100'})\n",
    "while not end_tag:\n",
    "    driver.execute_script('window.scrollBy(0, 10000)')\n",
    "    time.sleep(0.5)\n",
    "    html_2 = driver.page_source\n",
    "    soup2 = bs(html_2,'html.parser')\n",
    "    end_tag = soup2.find('div', {'class': 'jss100'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "end_tag = soup2.find('div', {'class': 'jss100'})\n",
    "print(end_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 获取网站2需加载的连接\n",
    "web_2_themes = []\n",
    "next_links = soup2.find_all('a', {'href': re.compile(\"^/thread.*\")})\n",
    "for next_link in next_links:\n",
    "    link = host_2+next_link['href']\n",
    "    title = ''.join(list(list(list(next_link.parent.children)[1].children)[1].children)[0].strings)\n",
    "    post_time = list(list(list(list(next_link.parent.children)[1].children)[0].children)[0].children)[1]['title']\n",
    "    author = ''.join(list(list(list(list(next_link.parent.children)[1].children)[0].children)[0].children)[0].strings)\n",
    "    theme = (author, title, post_time, link)\n",
    "    print(theme)\n",
    "    web_2_themes.append(theme)\n",
    "print(f'获取全部帖子链接完成，共计{len(web_2_themes)}个')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 对每个帖子进行加载\n",
    "page = 1\n",
    "for author, title, author_time, link in web_2_themes:\n",
    "    # 加载所有楼层\n",
    "    url_content = link+f'/page/{page}'\n",
    "    driver2 = webdriver.Chrome()\n",
    "    print(url_content)\n",
    "    try:\n",
    "        driver2.get(url_content)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    content = driver2.page_source\n",
    "    soup_content = bs(content,'html.parser')\n",
    "    texts = soup_content.find_all('div', {'data-role':'reply'})\n",
    "    while texts:\n",
    "        for text in texts:\n",
    "            replay_text = ''.join(list(text.children)[1].strings)\n",
    "            follow_time = list(list(list(list(list(text.children)[0].children)[0].children)[1].children)[1].children)[2]['title']\n",
    "            print(replay_text, follow_time)\n",
    "            # 保存数据\n",
    "            results['帖子标题'].append(title)\n",
    "            results['发帖人'].append(author)\n",
    "            results['发帖时间'].append(author_time)\n",
    "            results['link'].append(link)\n",
    "            results['评论'].append(replay_text)\n",
    "            results['评论时间'].append(follow_time)\n",
    "        time.sleep(5)\n",
    "        print()\n",
    "        page += 1\n",
    "        url_content = link+f'/page/{page}'\n",
    "        print(url_content)\n",
    "        try:\n",
    "            driver2.get(url_content)\n",
    "            content = driver2.page_source\n",
    "            soup_content = bs(content,'html.parser')\n",
    "            texts = soup_content.find_all('div', {'data-role':'reply'})\n",
    "        except Exception as e:\n",
    "            break\n",
    "    page = 1\n",
    "# driver2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.replace('',np.nan, inplace =True)\n",
    "df.dropna(subset='评论', inplace =True)\n",
    "df.to_excel(f'网站2结果_{keyword}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 网站三 uwants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 网站配置\n",
    "\n",
    "URL: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.replace('',np.nan, inplace =True)\n",
    "df.dropna(subset=['评论'], inplace =True)\n",
    "df.to_excel(f'网站2结果_{keyword}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 网站三 uwants"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}